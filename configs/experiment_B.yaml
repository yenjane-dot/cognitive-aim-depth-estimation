# Cognitive-Aim Model Training Configuration (Experiment B)
# Objective: Precise depth estimation + Target focusing + Environmental adaptation
description: "Cognitive-Aim model training (precise depth estimation + target focusing + cognitive awareness)"

model:
  # load_checkpoint: "checkpoints/precision_enhanced_train_best.pth"  # Commented out resume training config
  resume_checkpoint: false   # Train from scratch
  backbone: "dinov2"
  backbone_size: "base"
  use_lora: true
  lora_rank: 16
  cognitive_modules:
    - "ambient_stream"          # Ambient awareness stream - environmental assessment and background analysis
    - "iterative_focal_stream"  # Target focusing stream - precise targeting and depth measurement
    - "exif_prior_database"     # EXIF parameter database - environmental compensation and targeting experience
    # Future extension layers (demonstrating architectural scalability):
     # - "infrared_stream"        # Infrared data stream - thermal imaging targeting assistance
     # - "radar_stream"           # Radar data stream - distance measurement assistance
     # - "lidar_stream"           # LiDAR stream - precise distance perception
     # Note: Currently focused on pure visual targeting, maintaining simplicity and robustness
    # Openness: This architecture is designed to be modular, allowing easy addition of new layers without breaking core cognitive processes, demonstrating extensibility and flexibility for multimodal data.
  freeze_backbone: false
  
  # Cognitive awareness driven attention configuration - target focusing optimization
  curiosity_guided_attention:
    enable: true
    attention_heads: 8
    attention_dropout: 0.05  # Reduce dropout to avoid over-regularization of attention weights leading to uniform distribution
    
  # Variational Bayesian cognitive awareness module configuration - target focusing oriented
  enhanced_curiosity:
    enable: true
    hidden_dim: 256    # Fix dimension error: restore from 128 to 256, consistent with experiment A
    num_heads: 8
    dropout: 0.3     # Significantly increase dropout from 0.25 to 0.3, strengthen regularization
    hierarchical_levels: 4  # Increase hierarchical curiosity layers
    exploration_history_size: 1500  # Increase exploration history cache
    
    # Variational Bayesian architecture parameters
    variational_architecture:
      latent_dim_ratio: 4           # Latent space compression ratio
      encoder_layers: ["768->384", "384->192"]  # Encoder layer configuration
      decoder_layers: ["192->384", "384->192"]  # Decoder layer configuration
      use_batch_norm: true          # Use batch normalization
      activation: "relu"            # Activation function
      
    # Hierarchical weight configuration (focal stream priority version)
    geometric_weight: 0.3   # Moderately increase geometric curiosity weight
    local_weight: 0.9       # Maximize local curiosity weight (supporting focal stream)
    variational_weight: 0.4 # Significantly increase variational Bayesian weight, enhance uncertainty estimation
    focal_curiosity_boost: 5.0  # Significantly enhance focal stream curiosity boost factor
    
    # EXIF synergy enhancement - focal stream oriented
    exif_geometric_synergy: 0.4  # Reduce EXIF and geometric curiosity synergy weight
    exif_focal_synergy: 0.9      # Further enhance EXIF and focal stream synergy weight
    variational_exif_fusion: true  # Enable variational-EXIF fusion
    
  # EXIF configuration enhancement - focal stream priority fusion
  exif_config:
    enable_prior_fusion: true
    fusion_method: "focal_priority_weighted"  # Focal stream priority weighted fusion
    exif_embedding_dim: 128  # Further increase EXIF embedding dimension (enhance representation capability)
    prior_weight: 0.25      # Reduce EXIF weight, make way for focal stream
    focal_stream_weight: 0.5  # Focal stream weight (newly added)
    ambient_stream_weight: 0.25  # Global stream weight (reduced)
    num_cameras: 71
    # EXIF feature enhancement strategy
    exif_feature_enhancement:
      enable_geometric_encoding: true    # Enable geometric parameter encoding
      enable_depth_prior_fusion: true   # Enable depth prior fusion
      exif_attention_weight: 0.6        # EXIF attention weight
      focal_priority_boost: 1.5          # Focal stream priority boost factor
    
  # Iterative focusing enhancement - focal stream priority architecture
  focal_config:
    num_iterations: 6       # Keep 6 iterations, focus on quality rather than quantity
    focus_strength: 2.5     # Significantly enhance focusing strength, solve negative activation problem
    adaptive_focus: true    # Enable adaptive focusing
    activation_threshold: 0.0001  # Further reduce activation threshold
    gradient_flow_enhancement: true  # Enhance gradient flow
    focal_attention_weight: 2.0      # Significantly enhance focal attention weight
    focal_dominance_factor: 3.0      # Increase focal stream dominance factor
    iterative_amplification: 1.8     # Enhance iterative amplification coefficient

dataset:
  path: "dataset"
  image_size: 224
  augmentation: true

# Variational Bayesian cognitive awareness configuration - target focusing optimization
curiosity:
  enable: true             # Enable variational Bayesian cognitive awareness mechanism
  loss_type: "huber"       # Use Huber loss to improve robustness
  lambda: 0.5              # Significantly increase cognitive awareness weight Î», enhance synergistic exploration with focal stream
  warmup_epochs: 0         # Remove warmup period, immediately activate cognitive awareness
  uncertainty_weight: 0.6  # Significantly increase uncertainty weight from 0.4 to 0.6, strengthen threat identification
  
  # Variational Bayesian specific parameters
  variational_params:
    kl_weight: 0.15        # Enhance KL divergence weight (control regularization strength)
    latent_dim_ratio: 4    # Latent space dimension ratio (feature_dim // latent_dim_ratio)
    beta_schedule: "cosine" # Dynamic KL weight scheduling strategy (constant/linear/cosine)
    beta_start: 0.05       # Reduce KL weight start value
    beta_end: 1.5          # Increase KL weight end value
    reconstruction_weight: 1.2  # Increase reconstruction error weight
    
  # Hierarchical cognitive awareness configuration - variational Bayesian version
  hierarchical:
    enable: true
    level_weights: [0.2, 0.7, 0.1]  # [environmental awareness, target focusing, variational inference] - maximize target focusing weight
    adaptive_weighting: true
    exif_guided_weighting: true  # Enable EXIF parameter guided weight adjustment
    variational_integration: true  # Enable variational inference integration
    
  # Variational Bayesian statistics and visualization
  statistics:
    enable_logging: true
    log_frequency: 50       # More frequent logging
    save_attention_maps: true
    exploration_analysis: true
    # Variational Bayesian specific logging
    variational_logging:
      log_kl_divergence: true      # Log KL divergence
      log_reconstruction_error: true # Log reconstruction error
      log_latent_statistics: true   # Log latent space statistics
      latent_visualization_frequency: 10  # Visualize latent space every 10 epochs
      save_latent_distributions: true     # Save latent distribution plots

training:
  optimizer: "AdamW"
  learning_rate: 0.00002      # Reduce learning rate, stabilize gradient norm
  weight_decay: 0.015         # Enhance weight decay to control overfitting
  epochs: 150              # Increase training epochs
  batch_size: 128            # Significantly reduce batch size to improve precision
  accumulation_steps: 4   # Significantly increase gradient accumulation
  scheduler: "warmup_cosine"
  warmup_epochs: 0         # Remove warmup mechanism, immediately use full learning rate
  lr_patience: 10
  
  # Gradient optimization strategy - precision oriented
  grad_clip_type: "norm"   # Change to standard gradient clipping to ensure stability
  grad_clip_value: 1     # Further reduce clipping threshold to prevent gradient explosion
  use_amsgrad: true
  attention_entropy_weight: 0.01  # Attention entropy regularization weight, encourage attention concentration
  spatial_attention_weight: 0.1  # Spatial attention loss weight, encourage central regions to get more attention
  
  # Loss function optimization - precision priority
  loss_type: "huber"       # Use Huber loss
  loss_scale_factor: 8.0   # Increase loss scaling factor, enhance gradient signal
  smooth_l1_beta: 0.5      # Reduce smooth L1 parameter, focus more on small errors
  huber_delta: 0.5         # Reduce Huber loss parameter, improve small error sensitivity
  
  # Advanced regularization strategy - strengthen overfitting control
  use_weight_decay_schedule: true
  weight_decay_schedule_type: "cosine"
  final_weight_decay: 0.005    # Significantly increase final weight decay from 0.001 to 0.005
  label_smoothing: 0.08        # Enhance label smoothing from 0.05 to 0.08
  
  # Early stopping mechanism configuration - stricter precision requirements
  early_stopping:
    patience: 30             # Reduce patience value, stop overfitting earlier
    min_delta: 0.0001        # Increase improvement threshold, stricter requirements
    monitor: "val_loss"
    restore_best_weights: true
  save_every: 3              # More frequent saving
  validate_every: 2          # More frequent validation

# Unified logging configuration
logging:
  log_dir: "logs"
  tensorboard: true
  save_images: true
  num_save_images: 12        # Save more images
  
  # Cognitive awareness related logging - detailed recording
  curiosity:
    log_curiosity_stats: true
    log_attention_weights: true
    log_exploration_history: true
    save_curiosity_heatmaps: true
    log_geometric_curiosity: true  # Add environmental awareness logging
    log_target_locking: true       # Add target focusing logging
    log_distance_precision: true   # Add distance precision logging
    
validation:
  frequency: 2
  metrics:
    - "rmse"
    - "mae"
    - "abs_rel"
    - "sq_rel"
    - "log10"
    - "delta1"
    - "delta2"
    - "delta3"
    - "threshold_acc"         # Focus on threshold accuracy
  save_predictions: true
  detailed_analysis: true    # Enable detailed analysis
  
checkpoint:
  save_dir: "checkpoints"
  save_frequency: 3          # More frequent checkpoint saving
  keep_last_n: 5            # Keep more checkpoints
  auto_resume: true
  
# Advanced configuration - architectural advantages demonstration
advanced:
  # Dynamic sampling strategy - enable hard sample sampling
  dynamic_sampling:
    enable: true
    difficulty_threshold: 0.6  # Lower threshold, more hard samples
    adaptive_sampling: true    # Adaptive sampling
    
  # Attention visualization - demonstrate architectural advantages
  attention_visualization:
    enable: true
    save_frequency: 25         # More frequent visualization
    save_iterative_attention: true  # Save iterative attention
    
  # Exploration history analysis - curiosity mechanism analysis
  exploration_history:
    enable: true
    analysis_frequency: 50
    save_statistics: true
    detailed_curiosity_analysis: true
    
  # Architectural advantage analysis
  architecture_analysis:
    enable: true
    module_contribution_analysis: true  # Module contribution analysis
    cognitive_flow_visualization: true  # Cognitive flow visualization
    
# Experiment tracking
experiment:
  name: "cognitive_aim_precision"
  tags: ["cognitive_aim", "target_focusing", "depth_estimation", "cognitive_awareness"]
  notes: "Cognitive-Aim learning mechanism - precise target focusing and depth estimation based on cognitive awareness"
  version: "v4.0"
  comparison_baseline: "precision_enhanced_train"  # Compare with precision enhancement experiment
  
  # Cognitive-Aim experiment specific configuration
  cognitive_experiment:
    compare_with_photography: true    # Compare with device mode
    ablation_studies:
      - "without_cognitive_awareness"   # Without cognitive awareness ablation
      - "different_iteration_counts"   # Different iteration counts ablation
      - "focus_strength_effects"       # Focus strength effects ablation
    performance_validation:
      enable_precision_tracking: true   # Track precision metrics
      log_target_focusing: true         # Monitor target focusing
      measure_depth_accuracy: true  # Measure depth accuracy

# Cognitive-Aim special configuration
precision_optimization:
  # Cognitive-Aim precision enhancement
  cognitive_precision:
    enable: true
    target_accuracy_threshold: 0.05  # 5cm precision target
    iterative_refinement: true
    geometric_consistency_weight: 0.4
    depth_estimation_priority: true  # Prioritize depth estimation
    target_focusing_enhancement: 1.5      # Target focusing enhancement factor
    
  # Cognitive-Aim architectural advantages
  cognitive_advantage:
    multi_scale_fusion: true         # Multi-scale feature fusion
    experience_guided_learning: true # Experience guided learning
    cognitive_awareness_exploration: true # Cognitive awareness driven exploration
    adaptive_attention_mechanism: true # Adaptive attention mechanism
    environmental_adaptation: true    # Environmental adaptation capability
    uncertainty_assessment: true          # Uncertainty assessment capability